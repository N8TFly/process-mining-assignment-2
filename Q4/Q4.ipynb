{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "1. splitten op jaartal voordat Xtrain en Ytrain gemaakt worden (2015 train en dan 2016 test maken)\n",
    "2. je mag alleen data gebruiken die tot de XOR-split gebeurt is\n",
    "3. van te voren een split kiezen (accept/reject) doctype komt niet altijd vaker voor in dezelfde trace\n",
    "4. risk factor is altijd 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rowan/school/process-mining-assignment-2/../pydream\n",
      "/home/rowan/school/process-mining-assignment-2/pydream\n",
      "/home/rowan/school/process-mining-assignment-2/pydream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rowan/.local/lib/python3.8/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "module_path = str(Path.cwd().parents[0] / \"Q1\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import utils\n",
    "\n",
    "module_path2 = str(Path.cwd().parents[0] / \"Q4/PyDREAM/pydream\")\n",
    "if module_path2 not in sys.path:\n",
    "    sys.path.append(module_path2)\n",
    "    \n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_import_factory\n",
    "from pm4py.objects.petri.exporter.exporter import pnml as pnml_exporter\n",
    "from pm4py.objects.petri.importer.importer import pnml as pnml_importer\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "# from pm4py.objects.log.importer.csv import importer as csv_importer\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.log.util import get_log_representation\n",
    "\n",
    "from LogWrapper import LogWrapper\n",
    "from EnhancedPN import EnhancedPN\n",
    "from predictive.nap.NAP import NAP\n",
    "from predictive.nap.NAPr import NAPr\n",
    "from util.TimedStateSamples import loadTimedStateSamples\n",
    "\n",
    "from pm4py.visualization.petrinet import visualizer as pn_vis_factory\n",
    "from graphviz import *\n",
    "\n",
    "# Import utils.py from Q1 folder\n",
    "module_path = str(Path.cwd().parents[0] / \"Q1\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import utils\n",
    "\n",
    "from pm4py.visualization.petrinet import visualizer\n",
    "from pm4py.algo.enhancement.decision import algorithm as decision_mining\n",
    "\n",
    "# len(log.loc[log['Undesired Outcome 1'] == True])\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_categories_doctype_subprocess(dataframe):\n",
    "    \"\"\"Returns dataframe which has the doctype_subprocess value filled in\"\"\"\n",
    "    for idx, row in enumerate(tqdm(dataframe.values)):\n",
    "        value =  dataframe.loc[idx][\"doctype\"] + \"_\" + dataframe.loc[idx][\"subprocess\"]\n",
    "        dataframe.at[idx, \"doctype_subprocess\"] = value\n",
    "    \n",
    "    return dataframe \n",
    "\n",
    "def create_activity_categories(dataframe):\n",
    "    start = [\"income\", \"initialize\", \"begin\", \"create\"]\n",
    "    during = [\"editing\", \"performed\", \"calculate\", \"decide\", \"change\", \"correction\", \"revoke\", \"clear\", \"valid\", \"save\", \"plan\", \"prepare\", \"original\", \"check\", \"offline\", \"insert\", \"remove\", \"pre-check\"]\n",
    "    end = [\"finish\", \"abort\", \"discard\", \"withdraw\", \"approve\", \"refuse\"]\n",
    "    activity_type = [\"Payment\", \"Entitlement\", \"Control\", \"Parcel\", \"Geo\", \"Reference\", \"Department\", \"Inspection\"]\n",
    "    parsed = []\n",
    "    not_parsed = []\n",
    "    mapping = dict()\n",
    "    to_parse = deepcopy(dataframe[\"Activity\"].unique())\n",
    "\n",
    "    for idx, item in enumerate(tqdm(to_parse)):\n",
    "        filtered = False\n",
    "\n",
    "        for word in start:\n",
    "            if word in item:\n",
    "                for activity in activity_type:\n",
    "                    if activity in item:\n",
    "                        filtered = True\n",
    "                        mapping[to_parse[idx]] = \"Start_\" + activity\n",
    "                        parsed.append(to_parse[idx])\n",
    "                        break\n",
    "\n",
    "        for word in during:\n",
    "            if word in item:\n",
    "                for activity in activity_type:\n",
    "                    if activity in item:\n",
    "                        filtered = True\n",
    "                        mapping[to_parse[idx]] = \"During_\" + activity\n",
    "                        parsed.append(to_parse[idx])\n",
    "                        break\n",
    "\n",
    "        for word in end:\n",
    "            if word in item:\n",
    "                for activity in activity_type:\n",
    "                    if activity in item:\n",
    "                        filtered = True\n",
    "                        mapping[to_parse[idx]] = \"End_\" + activity\n",
    "                        parsed.append(to_parse[idx])\n",
    "                        break\n",
    "\n",
    "        if not filtered:\n",
    "            not_parsed.append(to_parse[idx])\n",
    "\n",
    "    if len(not_parsed) > 0:\n",
    "        raise ValueError(f\"Not all activities were correctly categorized as 'start', 'during', 'end' or were not given an 'activity_type'. \\n It concerns the following items: \\n \\n {not_parsed}\")\n",
    "    \n",
    "    total = len(set(val for dic in [mapping] for val in mapping.values()))\n",
    "    print(f\"There are {total} categories remaining.\")\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "def add_mapping_to_df(dataframe):\n",
    "    for idx, item in enumerate(tqdm(dataframe.values)):\n",
    "        dataframe.at[idx, \"Categorized_Activity\"] = mapping[dataframe.loc[idx][\"Activity\"]]\n",
    "        \n",
    "def save_results(miner_name, filename, results):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(miner_name)\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"Fitness: \" + str(results[0]))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"Generalization: \" + str(results[1]))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"Simplicity: \" + str(results[2]))\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "def analyze_petri(log, miner_name, net, initial_marking, final_marking, filter_type=\"doctype\"):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # fitness based on toked-based replay\n",
    "    fitness = replay_fitness_evaluator.apply(log, net, initial_marking, final_marking, \n",
    "              variant=replay_fitness_evaluator.Variants.TOKEN_BASED,\n",
    "              parameters={constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Categorized_Activity\"})\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    # precision based on token-based replay\n",
    "    #prec = precision_evaluator.apply(df2, net, initial_marking, final_marking, variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE, parameters={constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Categorized_Activity\"})\n",
    "\n",
    "    #print(f\"[ Finished Precision metric: {time.perf_counter() - start} sec]\")\n",
    "\n",
    "    # collect garbage\n",
    "    #gc.collect()\n",
    "\n",
    "    # generalization based on token-based replay\n",
    "    gen = generalization_evaluator.apply(log, net, initial_marking, final_marking, parameters={constants.PARAMETER_CONSTANT_CASEID_KEY: 'case:concept:name', constants.PARAMETER_CONSTANT_ACTIVITY_KEY: \"Categorized_Activity\", constants.PARAMETER_CONSTANT_TIMESTAMP_KEY: \"time:timestamp\"})\n",
    "                                         \n",
    "\n",
    "    # collect garbage\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"[ Finished Generalization metric: {time.perf_counter() - start} sec]\")\n",
    "\n",
    "    # simplicity measured by the inverse arc degree\n",
    "    simp = simplicity_evaluator.apply(net)\n",
    "\n",
    "    print(f\"[Finished: {time.perf_counter() - start}]\")\n",
    "    results = [fitness, gen, simp]\n",
    "    filename = miner_name + \"_results.txt\"\n",
    "    \n",
    "    # save petri net\n",
    "    petrinet_name = miner_name + \".pnml\"\n",
    "    print(f\"[Saving petrinet of {miner_name} to file :{petrinet_name}_doctype]\")\n",
    "    pnml_exporter.export_net(net, initial_marking, petrinet_name)\n",
    "    \n",
    "    # save results\n",
    "    print(f\"[Saving {miner_name} to file :{filename}_doctype]\")\n",
    "    save_results(miner_name, filename, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to reproduce\n",
    "1. pip3 install -r requirements.txt in the PyDream folder (installation for ortools will fail)\n",
    "2. pip3 install ortools==7.5.7466\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important task in customer journey optimization is    predicting the next activity in the journey. It helps for instance in preparing the business owner for the next interaction point with the application. In this part, you will start from one of the process models from Part I of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select two meaningful XOR splits in the process model you choose from Part \n",
    "    1. “meaningful” selection of the XOR depends on how you will be able to motivate it from the business perspective. You can select XOR splits with more than two outgoing arcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading FILTERED parquet version of original dataframe (need to run first part of 'transform_and_filter_df.ipynb' for this). \")\n",
    "df_log = utils.load_log_filtered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading FILTERED parquet version of original dataframe (need to run first part of 'transform_and_filter_df.ipynb' for this). \")\n",
    "df = utils.load_parquet_filtered()\n",
    "\n",
    "print(len(df[\"Case ID\"].unique()))\n",
    "\n",
    "df[\"Categorized_Activity\"] = \"\"\n",
    "df = df.reset_index()\n",
    "\n",
    "# load undesired info log \n",
    "log = utils._load_pickle(\"../dataset/complete_undesired_outcomes_df\")\n",
    "\n",
    "# merge dataframes\n",
    "df = pd.merge(df, log, how=\"inner\", on=\"Case ID\")\n",
    "\n",
    "print(len(df.loc[df['Undesired Outcome 1'] == True]))\n",
    "\n",
    "# Filter on Undesired Outcome 1\n",
    "df_1 = df.loc[df['Undesired Outcome 1'] == True]\n",
    "\n",
    "print(len(df_1[\"Case ID\"].unique()))\n",
    "\n",
    "\n",
    "# remove some columns and rename\n",
    "#df_1 = df_1[['Case ID', \"Activity\", 'time:timestamp', \"Categorized_Activity\", \"doctype\", \"concept:name\"]].sort_values('time:timestamp').rename(columns={\"Case ID\": \"case:concept:name\"})\n",
    "df_1 = df_1.rename(columns={\"Case ID\": \"case:concept:name\"})\n",
    "# reset index for changes\n",
    "df_1 = df_1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to reproduce\n",
    "1. pip3 install -r requirements.txt in the PyDream folder (installation for ortools will fail)\n",
    "2. pip3 install ortools==7.5.7466\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important task in customer journey optimization is    predicting the next activity in the journey. It helps for instance in preparing the business owner for the next interaction point with the application. In this part, you will start from one of the process models from Part I of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select two meaningful XOR splits in the process model you choose from Part \n",
    "    1. “meaningful” selection of the XOR depends on how you will be able to motivate it from the business perspective. You can select XOR splits with more than two outgoing arcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mapping\n",
    "mapping = create_activity_categories(df_1)\n",
    "\n",
    "# append column with new mapping to our old dataframe\n",
    "add_mapping_to_df(df_1)\n",
    "\n",
    "#output first 5 results\n",
    "df_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_1[\"case:concept:name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_on_year(dataframe, year):\n",
    "    \"\"\"Returns filtered dataframe\"\"\"\n",
    "    mask = (dataframe[\"time:timestamp\"] < str(year)).to_numpy()\n",
    "    return dataframe[mask]\n",
    "\n",
    "def filter_df_on_end_place(dataframe, activity):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_df_on_year(df_1, 2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = df_1.to_csv(\"df_1.csv\")\n",
    "\n",
    "# #event_stream = csv_importer.import_event_stream(os.path.join(\"tests\", \"input_data\", \"running-example.csv\"))\n",
    "log_csv = pd.read_csv('df_1.csv', sep=',')\n",
    "log_csv = dataframe_utils.convert_timestamp_columns_in_df(log_csv)\n",
    "log_csv = log_csv.sort_values('time:timestamp')\n",
    "event_log = log_converter.apply(log_csv)\n",
    "\n",
    "# # automatic feature selection on the event log\n",
    "# \"\"\"Results in 10 features that will be one hot encoded and used to traint the model\"\"\"\n",
    "# \"\"\" 8 out of 21 categories were considered important, together with index (so not timestamp) which is because\n",
    "# the traces were already sorted on timestamp. Lastly the Unnamed 0 feature from the event log. \n",
    "\n",
    "# \"\"\"\n",
    "# # data, feature_names = get_log_representation.get_default_representation(event_log)\n",
    "# # feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate a list of concept names\n",
    "\"\"\" Results in 36 unique concept:names\"\"\"\n",
    "activity_names = []\n",
    "for trace in event_log:\n",
    "    for event in trace:\n",
    "        if event[\"activity\"] in activity_names:\n",
    "            continue\n",
    "        else:\n",
    "            activity_names.append(event['activity'])\n",
    "\n",
    "print(f\"Gives {len(activity_names)} cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Per case, add trace information to the dictionary consisting of:\n",
    "    1. Young farmer\n",
    "    2. Area\n",
    "    3. Risk Factor\n",
    "    4. Number of parcels\n",
    "    5. Small famer\n",
    "    \n",
    "    Zijn al op tijd gesorteerd op time:timestamp bij het inladen van de dictionary\n",
    "    \n",
    "    Current Mapping:\n",
    "    [0: timestamp, \n",
    "     1: activity, \n",
    "     2: identifier, \n",
    "     3: categorized_activity, \n",
    "     4: young_farmer, \n",
    "     5: area, \n",
    "     6: risk_factor, \n",
    "     7: number_parcels, \n",
    "     8: small_farmer,\n",
    "     9: previous_activity,\n",
    "     10: next_activity]\n",
    "\"\"\"\n",
    "\n",
    "case_dict = {}\n",
    "df_1[\"Previous_Activity_OneHot\"] = \"\"\n",
    "df_1[\"Next_Activity_OneHot\"] = \"\"\n",
    "df_1[\"young_farmer\"] = 0\n",
    "\n",
    "for idx, row in tqdm(df_1.iterrows()):\n",
    "    young_farmer = 1 if row['(case) young farmer'] == True else 0\n",
    "    area = row['(case) area']\n",
    "    risk_factor = row['(case) risk_factor']\n",
    "    number_parcels = row['(case) number_parcels']\n",
    "    small_farmer = row['(case) small farmer']\n",
    "    timestamp = row['time:timestamp']\n",
    "    activity = row['activity']\n",
    "    identifier = row[\"level_0\"]\n",
    "    categorized_activity = row[\"Categorized_Activity\"]\n",
    "    \n",
    "    data = [timestamp, activity, identifier, categorized_activity, young_farmer, \n",
    "            area, risk_factor, number_parcels, small_farmer]\n",
    "    \n",
    "    if row[\"case:concept:name\"] in case_dict.keys():\n",
    "        case_dict[row[\"case:concept:name\"]].append(data)\n",
    "    else:\n",
    "        case_dict[row[\"case:concept:name\"]] = []\n",
    "        case_dict[row[\"case:concept:name\"]].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add one-hot for previous activity\"\"\"\n",
    "for sth, case in enumerate(tqdm(case_dict)):\n",
    "    for idx, event in enumerate(case_dict[case]):\n",
    "        if idx == 0:\n",
    "            previous_activity = [0 for i in range(36)]\n",
    "        else:\n",
    "            previous_activity = [1 if case_dict[case][idx-1][1] == value else 0 for value in activity_names]\n",
    "        event.append(previous_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add one-hot for next activity\"\"\"\n",
    "for sth, case in enumerate(tqdm(case_dict)):\n",
    "    for idx, event in enumerate(case_dict[case]):\n",
    "        if idx == len(case_dict[case])-1:\n",
    "            next_activity = [0 for i in range(36)]\n",
    "        else:\n",
    "            next_activity = [1 if case_dict[case][idx+1][1] == value else 0 for value in activity_names]\n",
    "        event.append(next_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add one-hot previous, next and young-farmer binary to the dataframe\"\"\"\n",
    "for case_id in tqdm(df_1[\"case:concept:name\"].unique()):\n",
    "    for idx in df_1.index[df_1[\"case:concept:name\"] == case_id]:\n",
    "        for event in case_dict[case_id]:\n",
    "            if event[2] == df_1.loc[idx][\"level_0\"]:\n",
    "                df_1.at[idx, \"Previous_Activity_OneHot\"] = event[-2]\n",
    "                df_1.at[idx, \"Next_Activity_OneHot\"] = event[-1]\n",
    "                df_1.at[idx, \"young_farmer\"] = 1 if event[4] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1.loc[df_1[\"case:concept:name\"] == \"99706b163f66b8ab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1[['Case ID', 'youngFarmer', \"activity\", \"area\", \"riskFactor\", \"young_farmer\", \"Previous_Activity_OneHot\"]].rename(\n",
    "    columns={'(case) risk_factor': \"riskFactor\", '(case) area':\"area\", '(case) number_parcels':\"parcels\", \n",
    "             '(case) young farmer':\"youngFarmer\", \n",
    "             '(case) small farmer':\"smallFarmer\", \"Case ID\":\"case:concept:name\"}) # , \"parcels\", \"activity\", \"Previous_Activity_OneHot\", \"Next_Activity_OneHot\"\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_1[:1].iterrows():\n",
    "    print(i[1][\"activity\"])\n",
    "#     if i[\"Previous_Activity_OneHot\"][0] == [0 for i in range(36)]:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_1[['(case) risk_factor', '(case) area', '(case) number_parcels',  'young_farmer']].to_numpy(dtype=object)\n",
    "# .rename(\n",
    "#     columns={'(case) risk_factor': \"riskFactor\", '(case) area':\"area\", '(case) number_parcels':\"parcels\", \n",
    "#              '(case) young farmer':\"youngFarmer\", \"case:concept:name\":\"Case ID\", \n",
    "#              '(case) small farmer':\"smallFarmer\"}).to_numpy() #\"Previous_Activity_OneHot\",\n",
    "Y = df_1[[\"activity\"]].to_numpy(dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "arr = range(0, 36)\n",
    "newY = []\n",
    "for idx, i in enumerate(Y):\n",
    "    ma_arr = ma.masked_array(arr, mask=df_1[\"Previous_Activity_OneHot\"][idx])\n",
    "    if len(ma_arr.data[ma_arr.mask]) == 0:\n",
    "        newY.append()\n",
    "    \n",
    "ma_arr = ma.masked_array(arr, mask=df_1[\"Previous_Activity_OneHot\"][3446])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_arr = ma.masked_array(arr, mask=df_1[\"Previous_Activity_OneHot\"][36])\n",
    "ma_arr.data[ma_arr.mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other way of adding information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1784873/1784873 [10:13<00:00, 2910.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# load filtered pandas df\n",
    "df = utils.load_parquet_filtered()\n",
    "df = df.rename(columns={\"Case ID\": \"case:concept:name\"})\n",
    "df = df.reset_index()\n",
    "\n",
    "\"\"\"Creates a column called doctype_subprocess with which the petri net was made\"\"\"\n",
    "df = create_categories_doctype_subprocess(df)\n",
    "\n",
    "# parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case'}\n",
    "#event_log = log_converter.apply(log_csv, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a pickle object\n",
    "df.to_pickle(\"./df_Q4.pkl\")\n",
    "\n",
    "df = pd.read_pickle(\"./df_Q4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as logfile\n",
    "log_csv = dataframe_utils.convert_timestamp_columns_in_df(df)\n",
    "log_csv = log_csv.sort_values('time:timestamp')\n",
    "event_log = log_converter.apply(log_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export log file TOO BIG!\n",
    "# from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "# xes_exporter.apply(event_log, 'log_for_Q4.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import log file\n",
    "# log = xes_importer.apply('log_for_Q4.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment application_Application': 35, 'Geo parcel document_Main': 1, 'Geo parcel document_Declared': 7, 'Control summary_Main': 3, 'Reference alignment_Main': 2, 'Department control parcels_Main': 1, 'label': 'Payment application_Application'} 2016-04-27 00:00:00+00:00\n",
      "There was no event after doctype_subprocess\n",
      "{'Payment application_Application': 27, 'Geo parcel document_Main': 1, 'Geo parcel document_Declared': 3, 'Control summary_Main': 3, 'Reference alignment_Main': 2, 'Department control parcels_Main': 1, 'label': 'Payment application_Application'} 2016-05-11 00:00:00+00:00\n",
      "There was no event after doctype_subprocess\n",
      "{'Payment application_Application': 24, 'Geo parcel document_Main': 8, 'Geo parcel document_Declared': 16, 'Control summary_Main': 3, 'Reference alignment_Main': 2, 'Department control parcels_Main': 4, 'label': 'Payment application_Application'} 2016-05-13 00:00:00+00:00\n",
      "There was no event after doctype_subprocess\n",
      "{'Payment application_Application': 34, 'Geo parcel document_Main': 4, 'Geo parcel document_Declared': 12, 'Control summary_Main': 7, 'Reference alignment_Main': 2, 'Department control parcels_Main': 7, 'label': 'Payment application_Application'} 2016-05-17 00:00:00+00:00\n",
      "There was no event after doctype_subprocess\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Calculate for each doctype_subprocess how many times it has occured before our target Split\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "\n",
    "def parse_case(case):\n",
    "    begin_payment = -1\n",
    "    features = {}\n",
    "    for i, e in enumerate(case):\n",
    "        a = e[\"doctype_subprocess\"]\n",
    "        if a == \"Payment application_Objection\": # Next event is the target\n",
    "            begin_payment = i\n",
    "            break\n",
    "        else:\n",
    "            features[a] = features.get(a, 0) + 1\n",
    "    features[\"label\"] = case[begin_payment+1][\"doctype_subprocess\"]\n",
    "    return features, case[begin_payment+1][\"time:timestamp\"]\n",
    "\n",
    "\n",
    "for case_idx, c in enumerate(event_log):\n",
    "    f = {}\n",
    "#     f[\"year\"] = c[0][\"time:timestamp\"].year\n",
    "    f[\"year\"] = c[0][\"(case) year\"]\n",
    "    try:\n",
    "        events, end_time = parse_case(c)\n",
    "        f.update(events)\n",
    "        f[\"duration\"] = end_time - c[0][\"time:timestamp\"]\n",
    "        f[\"duration\"] = f[\"duration\"].days\n",
    "        data.append(f)\n",
    "    except IndexError as e:\n",
    "        print(events, end_time)\n",
    "        print(\"There was no event after doctype_subprocess\")\n",
    "        \n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>Payment application_Application</th>\n",
       "      <th>Entitlement application_Main</th>\n",
       "      <th>Parcel document_Main</th>\n",
       "      <th>Control summary_Main</th>\n",
       "      <th>Reference alignment_Main</th>\n",
       "      <th>Department control parcels_Main</th>\n",
       "      <th>duration</th>\n",
       "      <th>Inspection_On-Site</th>\n",
       "      <th>Inspection_Remote</th>\n",
       "      <th>Payment application_Main</th>\n",
       "      <th>Entitlement application_Change</th>\n",
       "      <th>Geo parcel document_Main</th>\n",
       "      <th>Geo parcel document_Declared</th>\n",
       "      <th>Payment application_Change</th>\n",
       "      <th>Geo parcel document_Reported</th>\n",
       "      <th>Entitlement application_Objection</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Control summary_Main</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Department control parcels_Main</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entitlement application_Main</th>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geo parcel document_Declared</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geo parcel document_Main</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geo parcel document_Reported</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inspection_On-Site</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment application_Application</th>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "      <td>33515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment application_Objection</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reference alignment_Main</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  year  Payment application_Application  \\\n",
       "label                                                                     \n",
       "Control summary_Main                22                               22   \n",
       "Department control parcels_Main      1                                1   \n",
       "Entitlement application_Main       777                              777   \n",
       "Geo parcel document_Declared         8                                8   \n",
       "Geo parcel document_Main            10                               10   \n",
       "Geo parcel document_Reported         2                                2   \n",
       "Inspection_On-Site                   4                                4   \n",
       "Payment application_Application  33515                            33515   \n",
       "Payment application_Objection       71                               71   \n",
       "Reference alignment_Main             1                                1   \n",
       "\n",
       "                                 Entitlement application_Main  \\\n",
       "label                                                           \n",
       "Control summary_Main                                       22   \n",
       "Department control parcels_Main                             1   \n",
       "Entitlement application_Main                              777   \n",
       "Geo parcel document_Declared                                8   \n",
       "Geo parcel document_Main                                   10   \n",
       "Geo parcel document_Reported                                2   \n",
       "Inspection_On-Site                                          4   \n",
       "Payment application_Application                         33515   \n",
       "Payment application_Objection                              71   \n",
       "Reference alignment_Main                                    1   \n",
       "\n",
       "                                 Parcel document_Main  Control summary_Main  \\\n",
       "label                                                                         \n",
       "Control summary_Main                               22                    22   \n",
       "Department control parcels_Main                     1                     1   \n",
       "Entitlement application_Main                      777                   777   \n",
       "Geo parcel document_Declared                        8                     8   \n",
       "Geo parcel document_Main                           10                    10   \n",
       "Geo parcel document_Reported                        2                     2   \n",
       "Inspection_On-Site                                  4                     4   \n",
       "Payment application_Application                 33515                 33515   \n",
       "Payment application_Objection                      71                    71   \n",
       "Reference alignment_Main                            1                     1   \n",
       "\n",
       "                                 Reference alignment_Main  \\\n",
       "label                                                       \n",
       "Control summary_Main                                   22   \n",
       "Department control parcels_Main                         1   \n",
       "Entitlement application_Main                          777   \n",
       "Geo parcel document_Declared                            8   \n",
       "Geo parcel document_Main                               10   \n",
       "Geo parcel document_Reported                            2   \n",
       "Inspection_On-Site                                      4   \n",
       "Payment application_Application                     33515   \n",
       "Payment application_Objection                          71   \n",
       "Reference alignment_Main                                1   \n",
       "\n",
       "                                 Department control parcels_Main  duration  \\\n",
       "label                                                                        \n",
       "Control summary_Main                                          22        22   \n",
       "Department control parcels_Main                                1         1   \n",
       "Entitlement application_Main                                 777       777   \n",
       "Geo parcel document_Declared                                   8         8   \n",
       "Geo parcel document_Main                                      10        10   \n",
       "Geo parcel document_Reported                                   2         2   \n",
       "Inspection_On-Site                                             4         4   \n",
       "Payment application_Application                            33515     33515   \n",
       "Payment application_Objection                                 71        71   \n",
       "Reference alignment_Main                                       1         1   \n",
       "\n",
       "                                 Inspection_On-Site  Inspection_Remote  \\\n",
       "label                                                                    \n",
       "Control summary_Main                             22                 22   \n",
       "Department control parcels_Main                   1                  1   \n",
       "Entitlement application_Main                    777                777   \n",
       "Geo parcel document_Declared                      8                  8   \n",
       "Geo parcel document_Main                         10                 10   \n",
       "Geo parcel document_Reported                      2                  2   \n",
       "Inspection_On-Site                                4                  4   \n",
       "Payment application_Application               33515              33515   \n",
       "Payment application_Objection                    71                 71   \n",
       "Reference alignment_Main                          1                  1   \n",
       "\n",
       "                                 Payment application_Main  \\\n",
       "label                                                       \n",
       "Control summary_Main                                   22   \n",
       "Department control parcels_Main                         1   \n",
       "Entitlement application_Main                          777   \n",
       "Geo parcel document_Declared                            8   \n",
       "Geo parcel document_Main                               10   \n",
       "Geo parcel document_Reported                            2   \n",
       "Inspection_On-Site                                      4   \n",
       "Payment application_Application                     33515   \n",
       "Payment application_Objection                          71   \n",
       "Reference alignment_Main                                1   \n",
       "\n",
       "                                 Entitlement application_Change  \\\n",
       "label                                                             \n",
       "Control summary_Main                                         22   \n",
       "Department control parcels_Main                               1   \n",
       "Entitlement application_Main                                777   \n",
       "Geo parcel document_Declared                                  8   \n",
       "Geo parcel document_Main                                     10   \n",
       "Geo parcel document_Reported                                  2   \n",
       "Inspection_On-Site                                            4   \n",
       "Payment application_Application                           33515   \n",
       "Payment application_Objection                                71   \n",
       "Reference alignment_Main                                      1   \n",
       "\n",
       "                                 Geo parcel document_Main  \\\n",
       "label                                                       \n",
       "Control summary_Main                                   22   \n",
       "Department control parcels_Main                         1   \n",
       "Entitlement application_Main                          777   \n",
       "Geo parcel document_Declared                            8   \n",
       "Geo parcel document_Main                               10   \n",
       "Geo parcel document_Reported                            2   \n",
       "Inspection_On-Site                                      4   \n",
       "Payment application_Application                     33515   \n",
       "Payment application_Objection                          71   \n",
       "Reference alignment_Main                                1   \n",
       "\n",
       "                                 Geo parcel document_Declared  \\\n",
       "label                                                           \n",
       "Control summary_Main                                       22   \n",
       "Department control parcels_Main                             1   \n",
       "Entitlement application_Main                              777   \n",
       "Geo parcel document_Declared                                8   \n",
       "Geo parcel document_Main                                   10   \n",
       "Geo parcel document_Reported                                2   \n",
       "Inspection_On-Site                                          4   \n",
       "Payment application_Application                         33515   \n",
       "Payment application_Objection                              71   \n",
       "Reference alignment_Main                                    1   \n",
       "\n",
       "                                 Payment application_Change  \\\n",
       "label                                                         \n",
       "Control summary_Main                                     22   \n",
       "Department control parcels_Main                           1   \n",
       "Entitlement application_Main                            777   \n",
       "Geo parcel document_Declared                              8   \n",
       "Geo parcel document_Main                                 10   \n",
       "Geo parcel document_Reported                              2   \n",
       "Inspection_On-Site                                        4   \n",
       "Payment application_Application                       33515   \n",
       "Payment application_Objection                            71   \n",
       "Reference alignment_Main                                  1   \n",
       "\n",
       "                                 Geo parcel document_Reported  \\\n",
       "label                                                           \n",
       "Control summary_Main                                       22   \n",
       "Department control parcels_Main                             1   \n",
       "Entitlement application_Main                              777   \n",
       "Geo parcel document_Declared                                8   \n",
       "Geo parcel document_Main                                   10   \n",
       "Geo parcel document_Reported                                2   \n",
       "Inspection_On-Site                                          4   \n",
       "Payment application_Application                         33515   \n",
       "Payment application_Objection                              71   \n",
       "Reference alignment_Main                                    1   \n",
       "\n",
       "                                 Entitlement application_Objection  \n",
       "label                                                               \n",
       "Control summary_Main                                            22  \n",
       "Department control parcels_Main                                  1  \n",
       "Entitlement application_Main                                   777  \n",
       "Geo parcel document_Declared                                     8  \n",
       "Geo parcel document_Main                                        10  \n",
       "Geo parcel document_Reported                                     2  \n",
       "Inspection_On-Site                                               4  \n",
       "Payment application_Application                              33515  \n",
       "Payment application_Objection                                   71  \n",
       "Reference alignment_Main                                         1  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Get some descriptive statistics to see if calculating this makes sense\"\"\"\n",
    "df.sort_values(by=\"Payment application_Main\").max()\n",
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate random train-test sets based on all cases starting in 2015\"\"\"\n",
    "def train_test_split(dataframe, split_percentage=0.8, random_state=42):\n",
    "    train = dataframe.loc[dataframe['year'] == 2015]\n",
    "    X_train = train.sample(frac = split_percentage, replace = False, random_state=200)\n",
    "    X_test = train.drop(X_train.index)\n",
    "    \n",
    "    y_train = X_train.pop(\"label\").to_numpy()\n",
    "    y_test = X_test.pop(\"label\").to_numpy()\n",
    "    \n",
    "    \n",
    "    return X_train.to_numpy(), X_test.to_numpy(), y_train, y_test\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.015e+03, 1.300e+01, 1.900e+01, 7.000e+00, 4.000e+00, 2.000e+00,\n",
       "       1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform'): 0.8854101440200376\n",
      "SVC(C=0.025, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False): 0.9073262366938009\n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=2, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False): 0.8991859737006888\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(str(clf) + \": \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC / verhouding Precsion/recall ipv accuracy vanwege imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, clf, idx):\n",
    "    assert idx < len(df.loc[df['year'] == 2016]), \"Choose a smaller index value.\"\n",
    "    _input = df.loc[df['year'] == 2016].iloc[idx]\n",
    "    real_value = _input.pop(\"label\")\n",
    "    _input = _input.to_numpy().reshape(1, -1)\n",
    "    pred_val = clf.predict(_input)\n",
    "    \n",
    "    print(f\"Predicted value: {pred_val[0]}, \\n\\nReal value: {real_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Choose a smaller index value.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-334c10a6159b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m344424\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-176-feda44c54407>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(df, clf, idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2016\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Choose a smaller index value.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2016\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreal_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Choose a smaller index value."
     ]
    }
   ],
   "source": [
    "predict(df, clf, 344424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the heuristics miner petrinet from Q1\n",
    "net, initial_marking, final_marking = pnml_importer.import_net(\"../Q1/inductive_miner_undesired_outcome_2.pnml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gviz = visualizer.apply(net, initial_marking, final_marking, parameters={visualizer.Variants.WO_DECORATION.value.Parameters.DEBUG: True})\n",
    "visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "\n",
    "def parse_case(case):\n",
    "    begin_payment = -1\n",
    "    features = {}\n",
    "    for i, e in enumerate(case):\n",
    "        a = e[\"concept:name\"]\n",
    "        if a == \"begin payment\": # Next event is the target\n",
    "            begin_payment = i\n",
    "            break\n",
    "        else:\n",
    "            features[a] = features.get(a, 0) + 1\n",
    "    features[\"label\"] = case[begin_payment+1][\"concept:name\"]\n",
    "    return features, case[begin_payment+1][\"time:timestamp\"]\n",
    "\n",
    "\n",
    "for case_idx, c in enumerate(desired_outcome):\n",
    "    f = {}\n",
    "    f[\"year\"] = c[0][\"time:timestamp\"].year\n",
    "    try:\n",
    "        events, end_time = parse_case(c)\n",
    "        f.update(events)\n",
    "        f[\"duration\"] = end_time - c[0][\"time:timestamp\"]\n",
    "        f[\"duration\"] = f[\"duration\"].days\n",
    "        data.append(f)\n",
    "    except IndexError as e:\n",
    "        print(\"There was no event after begin payment\")\n",
    "        \n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyDREAM Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the one-hot encoding for use in the model\n",
    "def convert_ndarray_to_list(array, path='model-path/MODEL-NAME_nap_onehotdict.json'):\n",
    "    lists = array.tolist()\n",
    "    Path(path).touch(exist_ok=True)\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(lists, file)\n",
    "        \n",
    "convert_ndarray_to_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_event(self, df):\n",
    "        unique_case_ids = df[\"Case ID\"].unique()\n",
    "        for case in unique_case_ids:\n",
    "\n",
    "df.loc[df[\"Case ID\"] == \"8b99873a6136cfa6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, im, fm = heuristics_miner.apply(event_log, parameters={\"dependency_thresh\": 0.99})\n",
    "pnml_exporter.export_net(net, im, \"discovered_pn.pnml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the heuristics miner petrinet from Q1\n",
    "net, initial_marking, final_marking = pnml_importer.import_net(\"discovered_pn.pnml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance the log with decay functions\n",
    "log_wrapper = LogWrapper(event_log, resources=[\"Categorized_Activity\"])\n",
    "enhanced_pn = EnhancedPN(net, initial_marking)\n",
    "enhanced_pn.enhance(log_wrapper)\n",
    "enhanced_pn.saveToFile(\"enhanced_discovered_pn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open enhanced petri net\n",
    "enhanced_pn = EnhancedPN(net, initial_marking, decay_function_file=\"enhanced_discovered_pn.json\")\n",
    "tss_json, tss_objs = enhanced_pn.decay_replay(log_wrapper=log_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(enhanced_pn.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"timedstatesamples.json\", 'w') as fp:\n",
    "    json.dump(tss_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = NAPr(tss_train_file=\"timedstatesamples.json\", tss_test_file=\"timedstatesamples.json\", options={\"n_epochs\" : 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.train(checkpoint_path=\"model-path\", name=\"MODEL-NAME\", save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "algo.loadModel(path=\"model-path\", name=\"MODEL-NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values\n",
    "nap_out, string_out = algo.predict(tss_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_objs[4].data.items()\n",
    "print(dir(tss_objs[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize petri net\n",
    "gviz = visualizer.apply(enhanced_pn.net, initial_marking, final_marking, parameters={visualizer.Variants.WO_DECORATION.value.Parameters.DEBUG: True})\n",
    "visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transitions = net.transitions\n",
    "# print(len(transitions.difference()))\n",
    "# print(dir(transitions.__xor__(\"hid_142\")))\n",
    "enhanced_pn.net.places\n",
    "# for place in net.places:\n",
    "#     print(\"\\nPLACE: \"+ place.name)\n",
    "#     for arc in place.in_arcs:\n",
    "#         print(arc.source.name, arc.source.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_loaded_objs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_loaded_objs = loadTimedStateSamples(\"timedstatesamples.json\")\n",
    "nap_out, string_out = algo.predict([tss_loaded_objs[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "    log = xes_import_factory.apply('YOUR_EVENTLOG.xes')\n",
    "\n",
    "    net, im, fm = heuristics_miner.apply(log, parameters={\"dependency_thresh\": 0.99})\n",
    "    pnml_exporter.export_net(net, im, \"discovered_pn.pnml\")\n",
    "\n",
    "    net, initial_marking, final_marking = pnml_importer.import_net(\"discovered_pn.pnml\")\n",
    "\n",
    "    log_wrapper = LogWrapper(log)\n",
    "    enhanced_pn = EnhancedPN(net, initial_marking)\n",
    "    enhanced_pn.enhance(log_wrapper)\n",
    "    enhanced_pn.saveToFile(\"enhanced_discovered_pn.json\")\n",
    "\n",
    "    enhanced_pn = EnhancedPN(net, initial_marking, decay_function_file=\"enhanced_discovered_pn.json\")\n",
    "    tss_json, tss_objs = enhanced_pn.decay_replay(log_wrapper=log_wrapper)\n",
    "\n",
    "    with open(\"timedstatesamples.json\", 'w') as fp:\n",
    "        json.dump(tss_json, fp)\n",
    "\n",
    "    algo = NAP(tss_train_file=\"timedstatesamples.json\", tss_test_file=\"timedstatesamples.json\", options={\"n_epochs\" : 100})\n",
    "    algo.train(checkpoint_path=\"model-path\", name=\"MODEL-NAME\", save_results=True)\n",
    "\n",
    "    algo = NAP()\n",
    "    algo.loadModel(path=\"model-path\", name=\"MODEL-NAME\")\n",
    "\n",
    "    nap_out, string_out = algo.predict(tss_objs)\n",
    "\n",
    "    tss_loaded_objs = loadTimedStateSamples(\"timedstatesamples.json\")\n",
    "    nap_out, string_out = algo.predict(tss_loaded_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.enhancement.decision import algorithm as decision_mining\n",
    "X, y, class_names = decision_mining.apply(event_log, enhanced_pn.net, im, fm, decision_point=\"discard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf, feature_names, classes = decision_mining.get_decision_tree(df_1, net, im, fm, decision_point=\"pre_calculate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the mapping\n",
    "mapping = create_activity_categories(df_1)\n",
    "\n",
    "# append column with new mapping to our old dataframe\n",
    "add_mapping_to_df(df_1)\n",
    "\n",
    "#output first 5 results\n",
    "df_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv = df_1.to_csv(\"df_1.csv\")\n",
    "\n",
    "#event_stream = csv_importer.import_event_stream(os.path.join(\"tests\", \"input_data\", \"running-example.csv\"))\n",
    "log_csv = pd.read_csv('df_1.csv', sep=',')\n",
    "log_csv = dataframe_utils.convert_timestamp_columns_in_df(log_csv)\n",
    "log_csv = log_csv.sort_values('time:timestamp')\n",
    "event_log = log_converter.apply(log_csv)\n",
    "\n",
    "# automatic feature selection on the event log\n",
    "\"\"\"Results in 10 features that will be one hot encoded and used to traint the model\"\"\"\n",
    "\"\"\" 8 out of 21 categories were considered important, together with index (so not timestamp) which is because\n",
    "the traces were already sorted on timestamp. Lastly the Unnamed 0 feature from the event log. \n",
    "\n",
    "\"\"\"\n",
    "# data, feature_names = get_log_representation.get_default_representation(event_log)\n",
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_dict = {}\n",
    "for trace in event_log[0]:\n",
    "    print(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Unnamed: 0': 0, 'index': 25616, \n",
    " 'Activity': 'Payment application-Application-mail income', \n",
    " 'time:timestamp': Timestamp('2015-04-15 00:00:00+0000', tz='UTC'), \n",
    " 'Categorized_Activity': 'Start_Payment', 'doctype': 'Payment application', 'concept:name': 'mail income'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = []\n",
    "for trace in event_log:\n",
    "    for event in trace:\n",
    "        totals.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the one-hot encoding for use in the model\n",
    "def convert_ndarray_to_list(array, path='model-path/MODEL-NAME_nap_onehotdict.json'):\n",
    "    lists = array.tolist()\n",
    "    Path(path).touch(exist_ok=True)\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(lists, file)\n",
    "        \n",
    "convert_ndarray_to_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_event(self, df):\n",
    "        unique_case_ids = df[\"Case ID\"].unique()\n",
    "        for case in unique_case_ids:\n",
    "\n",
    "df.loc[df[\"Case ID\"] == \"8b99873a6136cfa6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, im, fm = heuristics_miner.apply(event_log, parameters={\"dependency_thresh\": 0.99})\n",
    "pnml_exporter.export_net(net, im, \"discovered_pn.pnml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the heuristics miner petrinet from Q1\n",
    "net, initial_marking, final_marking = pnml_importer.import_net(\"discovered_pn.pnml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhance the log with decay functions\n",
    "log_wrapper = LogWrapper(event_log, resources=[\"Categorized_Activity\"])\n",
    "enhanced_pn = EnhancedPN(net, initial_marking)\n",
    "enhanced_pn.enhance(log_wrapper)\n",
    "enhanced_pn.saveToFile(\"enhanced_discovered_pn.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open enhanced petri net\n",
    "enhanced_pn = EnhancedPN(net, initial_marking, decay_function_file=\"enhanced_discovered_pn.json\")\n",
    "tss_json, tss_objs = enhanced_pn.decay_replay(log_wrapper=log_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(enhanced_pn.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"timedstatesamples.json\", 'w') as fp:\n",
    "    json.dump(tss_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = NAPr(tss_train_file=\"timedstatesamples.json\", tss_test_file=\"timedstatesamples.json\", options={\"n_epochs\" : 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.train(checkpoint_path=\"model-path\", name=\"MODEL-NAME\", save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "algo.loadModel(path=\"model-path\", name=\"MODEL-NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values\n",
    "nap_out, string_out = algo.predict(tss_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_objs[4].data.items()\n",
    "print(dir(tss_objs[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize petri net\n",
    "gviz = visualizer.apply(enhanced_pn.net, initial_marking, final_marking, parameters={visualizer.Variants.WO_DECORATION.value.Parameters.DEBUG: True})\n",
    "visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transitions = net.transitions\n",
    "# print(len(transitions.difference()))\n",
    "# print(dir(transitions.__xor__(\"hid_142\")))\n",
    "enhanced_pn.net.places\n",
    "# for place in net.places:\n",
    "#     print(\"\\nPLACE: \"+ place.name)\n",
    "#     for arc in place.in_arcs:\n",
    "#         print(arc.source.name, arc.source.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_loaded_objs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_loaded_objs = loadTimedStateSamples(\"timedstatesamples.json\")\n",
    "nap_out, string_out = algo.predict([tss_loaded_objs[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "    log = xes_import_factory.apply('YOUR_EVENTLOG.xes')\n",
    "\n",
    "    net, im, fm = heuristics_miner.apply(log, parameters={\"dependency_thresh\": 0.99})\n",
    "    pnml_exporter.export_net(net, im, \"discovered_pn.pnml\")\n",
    "\n",
    "    net, initial_marking, final_marking = pnml_importer.import_net(\"discovered_pn.pnml\")\n",
    "\n",
    "    log_wrapper = LogWrapper(log)\n",
    "    enhanced_pn = EnhancedPN(net, initial_marking)\n",
    "    enhanced_pn.enhance(log_wrapper)\n",
    "    enhanced_pn.saveToFile(\"enhanced_discovered_pn.json\")\n",
    "\n",
    "    enhanced_pn = EnhancedPN(net, initial_marking, decay_function_file=\"enhanced_discovered_pn.json\")\n",
    "    tss_json, tss_objs = enhanced_pn.decay_replay(log_wrapper=log_wrapper)\n",
    "\n",
    "    with open(\"timedstatesamples.json\", 'w') as fp:\n",
    "        json.dump(tss_json, fp)\n",
    "\n",
    "    algo = NAP(tss_train_file=\"timedstatesamples.json\", tss_test_file=\"timedstatesamples.json\", options={\"n_epochs\" : 100})\n",
    "    algo.train(checkpoint_path=\"model-path\", name=\"MODEL-NAME\", save_results=True)\n",
    "\n",
    "    algo = NAP()\n",
    "    algo.loadModel(path=\"model-path\", name=\"MODEL-NAME\")\n",
    "38543\n",
    "    nap_out, string_out = algo.predict(tss_objs)\n",
    "\n",
    "    tss_loaded_objs = loadTimedStateSamples(\"timedstatesamples.json\")\n",
    "    nap_out, string_out = algo.predict(tss_loaded_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf, feature_names, classes = decision_mining.get_decision_tree(df_1, net, im, fm, decision_point=\"pre_calculate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
